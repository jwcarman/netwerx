# Netwerx

[![CI with Maven](https://github.com/jwcarman/netwerx/actions/workflows/maven.yml/badge.svg)](https://github.com/jwcarman/netwerx/actions/workflows/maven.yml)
[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=jwcarman_netwerx&metric=alert_status)](https://sonarcloud.io/summary/new_code?id=jwcarman_netwerx)
![License](https://img.shields.io/badge/License-Apache_2.0-blue)

[![Maintainability Rating](https://sonarcloud.io/api/project_badges/measure?project=jwcarman_netwerx&metric=sqale_rating)](https://sonarcloud.io/summary/new_code?id=jwcarman_netwerx)
[![Reliability Rating](https://sonarcloud.io/api/project_badges/measure?project=jwcarman_netwerx&metric=reliability_rating)](https://sonarcloud.io/summary/new_code?id=jwcarman_netwerx)
[![Security Rating](https://sonarcloud.io/api/project_badges/measure?project=jwcarman_netwerx&metric=security_rating)](https://sonarcloud.io/summary/new_code?id=jwcarman_netwerx)
[![Vulnerabilities](https://sonarcloud.io/api/project_badges/measure?project=jwcarman_netwerx&metric=vulnerabilities)](https://sonarcloud.io/summary/new_code?id=jwcarman_netwerx)
[![Coverage](https://sonarcloud.io/api/project_badges/measure?project=jwcarman_netwerx&metric=coverage)](https://sonarcloud.io/summary/new_code?id=jwcarman_netwerx)
[![Lines of Code](https://sonarcloud.io/api/project_badges/measure?project=jwcarman_netwerx&metric=ncloc)](https://sonarcloud.io/summary/new_code?id=jwcarman_netwerx)

---

**Netwerx** is a lightweight, extensible deep learning library for Java 23+.

Itâ€™s designed for learning, prototyping, and research â€” with full transparency into what your neural network is doing under the hood. No magic, no black boxes.

---

## ğŸ“‘ Table of Contents

* [âœ¨ Features](#-features)
* [ğŸš€ Quickstart](#-quickstart)
* [ğŸ“š Core Concepts](#-core-concepts)
* [ğŸ”Œ Activation Functions](#-activation-functions)
* [âš™ï¸ Optimizers](#ï¸-optimizers)
* [ğŸ¯ Loss Functions](#-loss-functions)
* [ğŸ§ª Training Executors](#-training-executors)
* [ğŸ“Š Scoring Functions](#-scoring-functions)
* [â¹ Early Stopping (Stopping Advisors)](#-early-stopping-stopping-advisors)
* [ğŸ›¡ Regularization](#-regularization)
* [ğŸ› Parameter Initialization](#-parameter-initialization)
* [ğŸ“£ Training Listeners](#-training-listeners)
* [ğŸ§  Model Types](#-model-types)
* [ğŸ§® Matrix Abstraction](#-matrix-abstraction)
* [ğŸ§ª Titanic Example](#-titanic-example)
* [ğŸ”§ Extending Netwerx](#-extending-netwerx)
* [ğŸ›¤ Roadmap](#-roadmap)
* [ğŸ¤ Contributing](#-contributing)
* [ğŸ“„ License](#-license)
* [ğŸ™ Acknowledgements](#-acknowledgements)

---

## âœ¨ Features

* Fully connected feed-forward networks
* Binary/multi-class classifiers, regressors, autoencoders
* Mini-batch or full-batch training
* Dropout support for regularization
* Modular components (optimizers, activations, loss functions, etc.)
* Pluggable matrix backend
* Lightweight â€” depends only on EJML
* Training listeners and early stopping
* Reproducibility with pluggable random sources

---

## ğŸš€ Quickstart

```java
var trainer = new DefaultNeuralNetworkTrainerBuilder<>(factory, 5)
        .defaultOptimizer(() -> Optimizers.adam(0.01))
        .denseLayer(layer -> layer.units(8).activationFunction(ActivationFunctions.relu()))
        .denseLayer(layer -> layer.units(1).activationFunction(ActivationFunctions.sigmoid()))
        .buildBinaryClassifierTrainer();

var trainFeatures = factory.filled(5, 10, 0.5);
var trainLabels = factory.filled(1, 10, 1.0);
var dataset = new Dataset<>(trainFeatures, trainLabels);
var network = trainer.train(dataset);
```

---

## ğŸ“š Core Concepts

* **Activation Functions** â€” introduce non-linearity into your network
* **Regularization** â€” penalize model complexity
* **Loss Functions** â€” define how wrong your model is
* **Optimizers** â€” control how weights are updated
* **Training Executors** â€” manage batching and parallel execution
* **Scoring Functions** â€” evaluate training progress
* **Stopping Advisors** â€” control when training halts
* **Parameter Initialization** â€” choose sensible starting points for weights and biases
* **Training Listeners** â€” observe and respond to training events
* **Matrix Abstraction** â€” plug in your own backend

---

## ğŸ”Œ Activation Functions

Netwerx supports ReLU, Sigmoid, Tanh, LeakyReLU, Softmax, and Linear. You can plug in your own:

```java
var relu = ActivationFunctions.relu();
var custom = (ActivationFunction) (input) -> ...
```

---

## âš™ï¸ Optimizers

Optimizers update your weights each step:

* **SGD** â€” basic gradient descent
* **Momentum** â€” adds inertia
* **Adam** â€” adaptive learning rate + momentum
* **RMSProp** â€” adaptive learning rate only

```java
Optimizers.adam(0.01, 0.9, 0.999, 1e-8);
```

---

## ğŸ¯ Loss Functions

Use a loss function suited to your task:

* **MSE** â€” regression
* **MAE** â€” regression
* **Binary Cross Entropy** â€” binary classification
* **Categorical Cross Entropy** â€” multi-class
* **Hinge** â€” SVM-style classifiers

```java
LossFunctions.bce();
```

---

## ğŸ§ª Training Executors

Training executors handle how training samples are fed to the network:

* **Full Batch**: use entire dataset each epoch
* **Mini Batch**: configurable size, shuffling, and parallelism

```java
TrainingExecutors.miniBatch(32, new Random(), Executors.newFixedThreadPool(4));
```

---

## ğŸ“Š Scoring Functions

Scoring functions monitor progress, typically by evaluating validation loss or accuracy. Use one of ours or create your own:

```java
ScoringFunctions.validationLoss();
```

---

## â¹ Early Stopping (Stopping Advisors)

Stop training when it's no longer improving:

* **Max Epochs**
* **Score Threshold**
* **Patience**

```java
StoppingAdvisors.patience(10, 1e-4);
```

---

## ğŸ›¡ Regularization

Avoid overfitting by penalizing weights:

* **L1** (sparsity)
* **L2** (shrinkage)
* **Elastic Net** (combines both)

```java
Regularizations.l2(1e-4);
```

---

## ğŸ› Parameter Initialization

Choose how weights and biases are initialized:

```java
ParameterInitializers.heUniform();
ParameterInitializers.zeros();
```

---

## ğŸ“£ Training Listeners

Attach listeners to monitor progress:

```java
TrainingListeners.logging(logger, 100);
```

Custom listeners can log metrics, write to disk, update UIs, etc.

---

## ğŸ§  Model Types

Netwerx supports:

* **BinaryClassifierTrainer** â€” one output, sigmoid, BCE loss
* **MultiClassifierTrainer** â€” softmax, categorical loss
* **RegressionTrainer** â€” identity output, MSE/MAE
* **AutoencoderTrainer** â€” encoder/decoder pattern with MSE loss

---

## ğŸ§® Matrix Abstraction

All computations are built on a pluggable matrix abstraction:

```java
Matrix<M> matrix = factory.random(rows, cols);
```

Plug in your own backend (e.g., EJML, ND4J) by implementing `Matrix<M>`.

---

## ğŸ§ª Titanic Example

A binary classifier predicts Titanic survival:

* Input: class, age, sex, fare, family members
* Layers: \[8 â†’ 4 â†’ 1]
* Activation: ReLU + Sigmoid
* Loss: Binary Cross Entropy
* Optimizer: SGD

```java
Accuracy: ~83%, F1 Score: 0.75
```

---

## ğŸ”§ Extending Netwerx

| You want to...         | Implement...                          |
| ---------------------- | ------------------------------------- |
| Add an activation      | `ActivationFunction`                  |
| Add a loss function    | `LossFunction`                        |
| Create an optimizer    | `Optimizer`                           |
| Add scoring/early stop | `ScoringFunction` / `StoppingAdvisor` |
| Monitor training       | `TrainingListener`                    |

---

## ğŸ›¤ Roadmap

* [x] Binary/Multi/Regression/Autoencoder Trainers
* [x] Dropout support
* [x] Mini-batch + parallel execution
* [x] Early stopping (patience, score threshold)
* [x] Adam, RMSProp, Momentum
* [x] Xavier, He initialization
* [ ] Model serialization
* [ ] Learning rate schedulers
* [ ] CNN, RNN layer support
* [ ] Visual training dashboards

---

## ğŸ¤ Contributing

Have an idea? Found a bug? Contributions are welcome!

* Fork, branch, submit PRs
* Add your own trainers, layers, components
* Suggest improvements via Issues

---

## ğŸ“„ License

Licensed under [Apache License 2.0](LICENSE)

---

## ğŸ™ Acknowledgements

Inspired by:

* PyTorch
* Keras
* TensorFlow

Built from scratch for Java developers who want to deeply understand whatâ€™s happening in a neural network.

---

# ğŸ§  Build neural networks. Understand them deeply. With **Netwerx**.
